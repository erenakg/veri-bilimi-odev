"""
Ä°stanbul Trafik DavranÄ±ÅŸ KalÄ±plarÄ±nÄ±n Analizi - Ana Dosya
Bu dosya projenin tÃ¼m adÄ±mlarÄ±nÄ± sÄ±rasÄ±yla Ã§alÄ±ÅŸtÄ±rÄ±r.
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import os

# Kendi modÃ¼llerimizi import edelim
import sys
sys.path.append('src')

from data_preprocessing import preprocess_data, save_processed_data
from clustering_analysis import determine_optimal_clusters, perform_kmeans_clustering
from visualization import plot_cluster_analysis, plot_pca_clusters, create_traffic_map, plot_cluster_characteristics

def create_directories():
    """Gerekli klasÃ¶rleri oluÅŸturur."""
    try:
        # Mevcut dosyalarÄ± sil, klasÃ¶rleri oluÅŸtur
        if os.path.exists('results/graphs') and os.path.isfile('results/graphs'):
            os.remove('results/graphs')
        if os.path.exists('results/maps') and os.path.isfile('results/maps'):
            os.remove('results/maps')
            
        os.makedirs('results', exist_ok=True)
        os.makedirs('results/graphs', exist_ok=True)
        os.makedirs('results/maps', exist_ok=True)
        print("âœ“ KlasÃ¶rler kontrol edildi")
    except Exception as e:
        print(f"KlasÃ¶r oluÅŸturma hatasÄ±: {e}")

def main():
    """Ana analiz fonksiyonu"""
    print("=== Ä°stanbul Trafik DavranÄ±ÅŸ KalÄ±plarÄ±nÄ±n Analizi ===\n")
    print("ğŸ“Š 1.7M veri ile tam analiz yapÄ±lÄ±yor...")
    
    # KlasÃ¶rleri oluÅŸtur
    create_directories()
    
    # 1. Veri YÃ¼kleme ve Ã–n Ä°ÅŸleme
    print("1. Veri yÃ¼kleniyor ve Ã¶n iÅŸleme yapÄ±lÄ±yor...")
    
    try:
        # Tam veri dosyasÄ±nÄ± yÃ¼kle
        raw_data = pd.read_csv('data/raw/traffic_density_202501.csv')
        print(f"Ham veri boyutu: {raw_data.shape}")
        print(f"Ham veri sÃ¼tunlarÄ±: {list(raw_data.columns)}")
        
        # Veri Ã¶n iÅŸleme (tam veri ile)
        processed_data = preprocess_data('data/raw/traffic_density_202501.csv')
        print(f"Ä°ÅŸlenmiÅŸ veri boyutu: {processed_data.shape}")
        
        # Ä°ÅŸlenmiÅŸ veriyi kaydet
        save_processed_data(processed_data, 'data/processed/temizlenmis_veri_tam.csv')
        print("âœ“ Veri Ã¶n iÅŸleme tamamlandÄ±\n")
        
    except Exception as e:
        print(f"Veri iÅŸleme hatasÄ±: {e}")
        return
    
    # 2. KÃ¼meleme iÃ§in Ã¶zellikleri hazÄ±rla
    print("2. KÃ¼meleme iÃ§in Ã¶zellikler hazÄ±rlanÄ±yor...")
    
    try:
        available_columns = processed_data.columns.tolist()
        print(f"Mevcut sÃ¼tunlar: {available_columns}")
        
        # KÃ¼meleme Ã¶zelliklerini seÃ§
        feature_columns = []
        
        # Ana trafik Ã¶zellikleri
        if 'AVERAGE_SPEED' in available_columns:
            feature_columns.append('AVERAGE_SPEED')
        if 'NUMBER_OF_VEHICLES' in available_columns:
            feature_columns.append('NUMBER_OF_VEHICLES')
        if 'MINIMUM_SPEED' in available_columns:
            feature_columns.append('MINIMUM_SPEED')
        if 'MAXIMUM_SPEED' in available_columns:
            feature_columns.append('MAXIMUM_SPEED')
            
        # Zaman Ã¶zellikleri
        if 'hour' in available_columns:
            feature_columns.append('hour')
        if 'day_of_week' in available_columns:
            feature_columns.append('day_of_week')
            
        print(f"KÃ¼meleme iÃ§in seÃ§ilen Ã¶zellikler: {feature_columns}")
        
        if len(feature_columns) == 0:
            print("âŒ KÃ¼meleme iÃ§in uygun Ã¶zellik bulunamadÄ±!")
            return
            
        X = processed_data[feature_columns].values
        
        # Veriyi normalize et
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        print(f"âœ“ Ã–zellikler hazÄ±rlandÄ±: {X_scaled.shape}")
        print(f"âš ï¸  Bu bÃ¼yÃ¼k veri ile iÅŸlem uzun sÃ¼rebilir...\n")
        
    except Exception as e:
        print(f"Ã–zellik hazÄ±rlama hatasÄ±: {e}")
        return
    
    # 3. Optimal kÃ¼me sayÄ±sÄ±nÄ± belirle
    print("3. Optimal kÃ¼me sayÄ±sÄ± belirleniyor...")
    print("â³ BÃ¼yÃ¼k veri ile kÃ¼me analizi yapÄ±lÄ±yor, lÃ¼tfen bekleyin...")
    
    try:
        # 1.7M veri iÃ§in kÃ¼me sayÄ±sÄ±nÄ± sÄ±nÄ±rla
        max_clusters = 6  # BÃ¼yÃ¼k veri iÃ§in makul limit
        
        print(f"ğŸ” Test edilecek kÃ¼me sayÄ±sÄ±: 2-{max_clusters}")
        
        inertia, silhouette_scores = determine_optimal_clusters(X_scaled, max_k=max_clusters)
        
        # Elbow ve Silhouette grafikleri
        plot_cluster_analysis(processed_data, inertia, silhouette_scores)
        
        # Optimal kÃ¼me sayÄ±sÄ±nÄ± belirle
        if len(silhouette_scores) > 0:
            optimal_k = np.argmax(silhouette_scores) + 2
        else:
            optimal_k = 4  # VarsayÄ±lan
            
        print(f"âœ“ Optimal kÃ¼me sayÄ±sÄ±: {optimal_k}\n")
        
    except Exception as e:
        print(f"KÃ¼me sayÄ±sÄ± belirleme hatasÄ±: {e}")
        optimal_k = 4
        print(f"VarsayÄ±lan kÃ¼me sayÄ±sÄ± kullanÄ±lÄ±yor: {optimal_k}\n")
    
    # 4. K-Means kÃ¼meleme uygula
    print("4. K-Means kÃ¼meleme uygulanÄ±yor...")
    print("â³ 1.7M veri ile kÃ¼meleme yapÄ±lÄ±yor, bu iÅŸlem 10-15 dakika sÃ¼rebilir...")
    
    try:
        cluster_labels = perform_kmeans_clustering(X_scaled, optimal_k)
        processed_data['cluster'] = cluster_labels
        
        print(f"âœ“ {optimal_k} kÃ¼me oluÅŸturuldu")
        print(f"KÃ¼me daÄŸÄ±lÄ±mÄ±:")
        cluster_counts = processed_data['cluster'].value_counts().sort_index()
        print(cluster_counts)
        print()
        
    except Exception as e:
        print(f"KÃ¼meleme hatasÄ±: {e}")
        return
    
    # 5. SonuÃ§larÄ± gÃ¶rselleÅŸtir
    print("5. SonuÃ§lar gÃ¶rselleÅŸtiriliyor...")
    print("â³ BÃ¼yÃ¼k veri gÃ¶rselleÅŸtirmesi yapÄ±lÄ±yor...")
    
    try:
        # PCA ile kÃ¼meleri gÃ¶rselleÅŸtir (Ã¶rnekleme ile)
        print("ğŸ“Š PCA analizi iÃ§in veri Ã¶rneklemesi yapÄ±lÄ±yor...")
        pca_sample_size = min(50000, len(processed_data))
        pca_sample_data = processed_data.sample(n=pca_sample_size, random_state=42)
        pca_sample_features = X_scaled[pca_sample_data.index]
        pca_sample_labels = cluster_labels[pca_sample_data.index]
        
        plot_pca_clusters(pca_sample_data, pca_sample_features, pca_sample_labels)
        
        # KÃ¼me Ã¶zelliklerini analiz et (tam veri ile)
        cluster_stats = plot_cluster_characteristics(processed_data, feature_columns)
        if cluster_stats is not None:
            print("KÃ¼me Ä°statistikleri:")
            print(cluster_stats)
            print()
        
        # Harita oluÅŸtur (koordinat bilgisi varsa - Ã¶rnekleme ile)
        if 'LATITUDE' in processed_data.columns and 'LONGITUDE' in processed_data.columns:
            print("ğŸ—ºï¸  Harita iÃ§in veri Ã¶rneklemesi yapÄ±lÄ±yor...")
            map_sample_size = min(10000, len(processed_data))  # Harita iÃ§in 10K nokta
            map_data = processed_data.sample(n=map_sample_size, random_state=42)
            
            # SÃ¼tun isimlerini dÃ¼zelt
            map_data = map_data.rename(columns={'LATITUDE': 'latitude', 'LONGITUDE': 'longitude'})
            traffic_map = create_traffic_map(map_data)
        else:
            print("âš ï¸ Koordinat bilgisi bulunamadÄ±, harita oluÅŸturulamadÄ±")
            
        print("âœ“ GÃ¶rselleÅŸtirme tamamlandÄ±\n")
        
    except Exception as e:
        print(f"GÃ¶rselleÅŸtirme hatasÄ±: {e}")
    
    # 6. KÃ¼me yorumlarÄ±
    print("6. KÃ¼me YorumlarÄ±:")
    print("="*60)
    
    try:
        for cluster_id in range(optimal_k):
            cluster_data = processed_data[processed_data['cluster'] == cluster_id]
            
            print(f"\nğŸ”µ KÃœME {cluster_id}:")
            print(f"  ğŸ“Š Veri NoktasÄ± SayÄ±sÄ±: {len(cluster_data):,}")
            print(f"  ğŸ“ˆ Toplam Veri OranÄ±: %{(len(cluster_data)/len(processed_data)*100):.1f}")
            
            # Ana Ã¶zellikler iÃ§in ortalama
            for feature in feature_columns:
                if feature in cluster_data.columns:
                    avg_value = cluster_data[feature].mean()
                    if 'SPEED' in feature:
                        print(f"  ğŸš— Ortalama {feature}: {avg_value:.1f} km/h")
                    elif 'VEHICLES' in feature:
                        print(f"  ğŸš™ Ortalama {feature}: {avg_value:.0f} araÃ§")
                    elif feature == 'hour':
                        print(f"  ğŸ• Ortalama Saat: {avg_value:.1f}")
                    else:
                        print(f"  ğŸ“‹ Ortalama {feature}: {avg_value:.2f}")
            
            # Trafik kalÄ±bÄ±nÄ± belirle
            if 'AVERAGE_SPEED' in cluster_data.columns:
                avg_speed = cluster_data['AVERAGE_SPEED'].mean()
                avg_vehicles = cluster_data['NUMBER_OF_VEHICLES'].mean() if 'NUMBER_OF_VEHICLES' in cluster_data.columns else 0
                
                if avg_speed < 30 and avg_vehicles > 150:
                    pattern = "ğŸ”´ YoÄŸun-SÄ±kÄ±ÅŸÄ±k Trafik"
                elif avg_speed > 50 and avg_vehicles < 100:
                    pattern = "ğŸŸ¢ AkÄ±cÄ±-DÃ¼ÅŸÃ¼k YoÄŸunluk"
                elif 30 <= avg_speed <= 50:
                    pattern = "ğŸŸ¡ Orta YoÄŸunluk Trafik"
                else:
                    pattern = "âšª Karma Trafik"
                    
                print(f"  ğŸ¯ Trafik KalÄ±bÄ±: {pattern}")
                
    except Exception as e:
        print(f"KÃ¼me yorumlama hatasÄ±: {e}")
    
    print("\n" + "="*60)
    print("ğŸ‰ === 1.7M Veri ile Analiz TamamlandÄ±! ===")
    print("ğŸ“ SonuÃ§lar 'results' klasÃ¶rÃ¼nde kaydedildi.")
    print("\nğŸ“‹ OluÅŸturulan dosyalar:")
    print("  ğŸ“Š results/graphs/cluster_analysis.png")
    print("  ğŸ¨ results/graphs/pca_clusters.png") 
    print("  ğŸ“ˆ results/graphs/cluster_characteristics.png")
    print("  ğŸ—ºï¸ results/maps/traffic_clusters.html")
    print("  ğŸ’¾ data/processed/temizlenmis_veri_tam.csv")

if __name__ == "__main__":
    main()