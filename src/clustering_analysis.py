import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import numpy as np

def load_data(file_path):
    data = pd.read_csv(file_path)
    return data

def preprocess_data(data):
    # Ã–zniteliklerin seÃ§ilmesi ve normalizasyonu
    features = data[['ortalama_hiz', 'arac_sayisi']]
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(features)
    return scaled_features

def determine_optimal_clusters(data, max_k=6):
    """1.7M veri iÃ§in optimize edilmiÅŸ kÃ¼me analizi."""
    print(f"ğŸ” KÃ¼meleme analizi baÅŸlÄ±yor...")
    print(f"   ğŸ“Š Tam veri boyutu: {data.shape}")
    print(f"   ğŸ¯ Test edilecek kÃ¼me sayÄ±sÄ±: 2-{max_k}")
    print(f"   âš ï¸  BÃ¼yÃ¼k veri nedeniyle kÃ¼meleme iÃ§in Ã¶rnekleme yapÄ±lÄ±yor...")
    
    # BÃ¼yÃ¼k veri iÃ§in kÃ¼meleme analizi Ã¶rneklemesi
    sample_size = min(200000, len(data))  # 200K ile kÃ¼me analizi
    sample_indices = np.random.choice(len(data), size=sample_size, replace=False)
    data_sample = data[sample_indices]
    print(f"   âœ“ {len(data_sample):,} veri noktasÄ± ile kÃ¼me analizi yapÄ±lacak")
    
    inertia = []
    silhouette_scores = []
    
    for k in range(2, max_k + 1):
        print(f"   ğŸ”„ K={k} test ediliyor...", end="", flush=True)
        try:
            # BÃ¼yÃ¼k veri iÃ§in hÄ±zlandÄ±rÄ±lmÄ±ÅŸ ayarlar
            kmeans = KMeans(
                n_clusters=k, 
                random_state=42, 
                n_init=5,  # HÄ±zlandÄ±rma iÃ§in azaltÄ±ldÄ±
                max_iter=200,  # Yeterli iterasyon
                tol=1e-3,  # GevÅŸetilmiÅŸ tolerans
                algorithm='lloyd'  # 'auto' yerine 'lloyd' veya 'elkan' kullanÄ±labilir
            )
            kmeans.fit(data_sample)
            
            inertia_val = kmeans.inertia_
            silhouette_val = silhouette_score(data_sample, kmeans.labels_)
            
            inertia.append(inertia_val)
            silhouette_scores.append(silhouette_val)
            
            print(f" âœ“ (Inertia: {inertia_val:.0f}, Silhouette: {silhouette_val:.3f})")
            
        except Exception as e:
            print(f" âŒ Hata: {e}")
            break
    
    print(f"âœ… KÃ¼meleme analizi tamamlandÄ±!")
    return inertia, silhouette_scores

def plot_elbow_method(inertia):
    plt.figure(figsize=(10, 6))
    plt.plot(range(2, len(inertia) + 2), inertia, marker='o')
    plt.title('Elbow Method for Optimal k')
    plt.xlabel('Number of clusters (k)')
    plt.ylabel('Inertia')
    plt.grid()
    plt.show()

def plot_silhouette_scores(silhouette_scores):
    plt.figure(figsize=(10, 6))
    plt.plot(range(2, len(silhouette_scores) + 2), silhouette_scores, marker='o')
    plt.title('Silhouette Scores for Optimal k')
    plt.xlabel('Number of clusters (k)')
    plt.ylabel('Silhouette Score')
    plt.grid()
    plt.show()

def perform_kmeans_clustering(data, n_clusters):
    """1.7M veri iÃ§in optimize edilmiÅŸ K-Means kÃ¼meleme."""
    print(f"ğŸ”„ K-Means kÃ¼meleme baÅŸlÄ±yor (K={n_clusters})...")
    print(f"   ğŸ“Š Tam veri boyutu: {data.shape}")
    print(f"   â³ Bu iÅŸlem 10-15 dakika sÃ¼rebilir...")
    
    # 1.7M veri iÃ§in optimizasyon ayarlarÄ±
    kmeans = KMeans(
        n_clusters=n_clusters, 
        random_state=42, 
        n_init=5,  # HÄ±zlandÄ±rma iÃ§in
        max_iter=300,
        tol=1e-3,
        algorithm='auto'  # En uygun algoritmayÄ± seÃ§er
    )
    
    # Tam veri ile kÃ¼meleme
    labels = kmeans.fit_predict(data)
    
    print(f"âœ… 1.7M veri ile kÃ¼meleme tamamlandÄ±!")
    return labels

# Ã–rnek kullanÄ±m
if __name__ == "__main__":
    raw_data_path = '../data/processed/temizlenmis_veri.csv'
    data = load_data(raw_data_path)
    scaled_data = preprocess_data(data)
    
    inertia, silhouette_scores = determine_optimal_clusters(scaled_data)
    plot_elbow_method(inertia)
    plot_silhouette_scores(silhouette_scores)
    
    optimal_k = 3  # Ã–rnek olarak belirlenen optimal kÃ¼me sayÄ±sÄ±
    labels = perform_kmeans_clustering(scaled_data, optimal_k)
    data['cluster'] = labels
    print(data.head())